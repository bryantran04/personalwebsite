(this.webpackJsonpbtran=this.webpackJsonpbtran||[]).push([[0],{13:function(e,t,a){e.exports=a(23)},23:function(e,t,a){"use strict";a.r(t);var n=a(0),r=a.n(n),c=a(9),i=a.n(c),o=a(1),l=a(2),s=a(4),u=a(3),m=function(e){Object(s.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(l.a)(a,[{key:"render",value:function(){var e={width:"15%",height:"15%",margin:"auto"};return r.a.createElement("nav",{id:"sidebar"},r.a.createElement("div",{class:"sidebar-header"},r.a.createElement("h3",null,"Bryan Tran")),r.a.createElement("ul",{class:"list-unstyled components"},r.a.createElement("li",null,r.a.createElement("a",{href:"#showcase"},"Introduction")),r.a.createElement("li",null,r.a.createElement("a",{href:"#about"},"About")),r.a.createElement("li",null,r.a.createElement("a",{href:"#projects"},"Projects")),r.a.createElement("li",null,r.a.createElement("a",{href:"documents/bryan_tran_resume.pdf",target:"_blank",rel:"noopener noreferrer"},"Resume"))),r.a.createElement("div",{className:"links"}),r.a.createElement("div",{style:{position:"absolute",bottom:"0",textAlign:"center",width:"100%"}},r.a.createElement("ul",{style:{listStyleType:"none",paddingInlineStart:"0px"}},r.a.createElement("li",null,r.a.createElement("a",{href:"https://www.linkedin.com/in/bryantranva/"},r.a.createElement("img",{src:"images/linkedin-brands.svg",style:e}))),r.a.createElement("li",null,r.a.createElement("a",{href:"https://github.com/bryantran04"},r.a.createElement("img",{src:"images/github-square-brands.svg",style:e})))),r.a.createElement("p",null,"   Made with React :)")))}}]),a}(n.Component),d=a(10),p=a.n(d),h=a(11),f=a.n(h),g=function(e){Object(s.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(l.a)(a,[{key:"render",value:function(){return r.a.createElement(p.a,null,r.a.createElement("div",{id:"showcase"},r.a.createElement(f.a,null),r.a.createElement("div",{className:"showcase-content"},r.a.createElement("h1",null,"Hello, I'm Bryan"),r.a.createElement("h2",null," I am a ",r.a.createElement("span",{class:"txt-rotate","data-period":"2000","data-rotate":'["Software Engineer", "Photographer", "ML Enthusiast"]'})))))}}]),a}(n.Component),E=a(12),v=[{img:"cap1alt.png",role:"Associate Software Engineer",years:"Aug 2020 - Present",desc:"I am a software engineer for Capital One, part of the TDP rotational program. I have build AWS infrastrcture to move our credit card information from old legacy mainframes onto our cloud database. I have also helped create full stack applications to help comply with data privacy laws. Backend in Python Fast API, frontend in React, and recommendation engine in Java Spring Boot / Kotlin.",location:"McLean, VA",company:"Capital One"},{img:"uva.jpg",role:"Graduated",years:"Aug 2016- May 2020",desc:"I studied Computer Science and Statistics.",location:"Charlottesville, VA",company:"University of Virginia"},{img:"cvent.jpg",role:"Software Engineering Intern",years:"June 2019 - Aug 2019",desc:"Testing Framework Development: Developed Java functions with Selenium for the company's testing framework to automate testing scripts and extract information from web pages. Used JIRA to track issues with the company's testing framework and Jenkins for continuous integration. Developed test cases with Cucumber and Gherkin to test the Java functions. \n API Endpoint Conversion: Re - factored old test cases that uses the UI to generate test data, to use API Endpoints instead.Reduced time in creating testing data sets and solve sync issues.Used Postman to test the converted scripts.Decreased failed tests by 50 % and improved overall execution time.",location:"McLean, VA",company:"Cvent"},{img:"sp.png",role:"Software Engineering Intern",years:"June 2018 - Aug 2018",desc:"Natural Language Processing: Developed Machine-Learning models with Python and Natural Language processing to analyze text and predict the categories of financial documents. Each financial document would then be sent to a specific team based on the category. Utilized scikit-learn and Tensorflow to build the SVM and deep-learning models. Pandas, Numpy, and matplotlib for exploratory data analysis. \n Data Pipeline: Implemented a text transformation through a pipeline in the final product.Text would be transformed based on TF - IDF and bag of words techniques.Used NTLK, numpy, and scikit - learn python packages.Entire service turned a week long task to approximately one hour.",location:"Charlottesville, VA",company:"Standard and Poor's Market Intelligence"}];function b(e){var t=r.a.useState(!0),a=Object(E.a)(t,2),n=a[0],c=a[1],i=r.a.useRef();return r.a.useEffect((function(){var e=new IntersectionObserver((function(e){e.forEach((function(e){return c(e.isIntersecting)}))}));return e.observe(i.current),function(){return e.unobserve(i.current)}}),[]),r.a.createElement("div",{className:"fade-in-section ".concat(n?"is-visible":""),ref:i},e.children)}var y=function(e){Object(s.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(l.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{id:"about",className:"subsection"},r.a.createElement("h1",null,"About"),r.a.createElement("div",{className:"container"},r.a.createElement(n.Fragment,null,v.map((function(e,t){return r.a.createElement(b,null,r.a.createElement("div",{class:"row mt-5 ",key:t},r.a.createElement("div",{className:"col-md-6 img-container"},r.a.createElement("img",{className:"about-img",src:"images/".concat(e.img)})),r.a.createElement("div",{className:"col-md-6"},r.a.createElement("h2",null,"".concat(e.company)),r.a.createElement("h5",null,"".concat(e.role)),r.a.createElement("h6",null,"".concat(e.location)),r.a.createElement("h6",null,"".concat(e.years)),r.a.createElement("p",null,"".concat(e.desc)))))})))))}}]),a}(n.Component),w=[{img:"furniture.jpg",title:"UVA Furniture",desc:"Developed a scalable web app with Django to help students find furniture. Used Docker to containerize micro-services, experience services, and front-end services. Used Kafka to queue item creation and queue search indexer and Elastic Search to optimize our search results. Used HaProxy for load balancing and Redis for caching.",link:"https://github.com/bryantran04"},{img:"torch-nn.png",title:"Picture2Emotion2Song",desc:"Web app that takes a picture, inputs into our pretrained model, and then outputs a spotify playlist. We used deep-learning to extract abstract emotion from various pictures. We fine tuned a VGG-16 model with pytorch and used flash with Spotify's web API",link:"documents/cvproj.pdf"}],k=function(e){Object(s.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(l.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{id:"projects",className:"subsection"},r.a.createElement("h1",null,"Projects"),r.a.createElement("div",{className:"container"},r.a.createElement("div",{class:"row mt-5"},w.map((function(e,t){return r.a.createElement("div",{className:"col-md-6"},r.a.createElement("div",{class:"card",style:{width:"22rem",height:"40rem"}},r.a.createElement("a",{href:"".concat(e.link),target:"_blank"},r.a.createElement("img",{className:"project-img",src:"images/".concat(e.img)})),r.a.createElement("div",{class:"card-body"},r.a.createElement("h5",{class:"card-title"},"".concat(e.title)),r.a.createElement("p",{class:"card-text"},"".concat(e.desc)))))})))))}}]),a}(n.Component),j=(a(8),function(e){Object(s.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(l.a)(a,[{key:"render",value:function(){return r.a.createElement("div",{className:"wrapper"},r.a.createElement(m,null),r.a.createElement("div",{id:"content"},r.a.createElement(g,null),r.a.createElement(y,null),r.a.createElement(k,null)))}}]),a}(r.a.Component));i.a.render(r.a.createElement(j,null),document.querySelector("#root"))},8:function(e,t,a){}},[[13,1,2]]]);
//# sourceMappingURL=main.0e077c5b.chunk.js.map